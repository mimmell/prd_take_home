{"doc_id": "2103.00020v1.pdf", "source_path": "../data/test_docs/2103.00020v1.pdf", "fields": {"authors": [], "abstract_summary": "State-of-the-art computer vision systems are\ntrained to predict a \ufb01xed set of predetermined\nobject categories. This restricted form of super-\nvision limits their generality and usability since\nadditional labeled data is needed to specify any\nother visual concept. Learning directly from raw\ntext about images is a promising alternative which\nleverages a much broader source of supervision.\nWe demonstrate that the simple pre-training task\nof predicting which caption goes with which im-\nage is an ef\ufb01...", "code_snippets": []}, "errors": []}
{"doc_id": "2106.09685v2.pdf", "source_path": "../data/test_docs/2106.09685v2.pdf", "fields": {"authors": [], "abstract_summary": "", "code_snippets": []}, "errors": []}
{"doc_id": "2006.03654v6.pdf", "source_path": "../data/test_docs/2006.03654v6.pdf", "fields": {"authors": [], "abstract_summary": "", "code_snippets": []}, "errors": []}
{"doc_id": "2005.04611v1.pdf", "source_path": "../data/test_docs/2005.04611v1.pdf", "fields": {"authors": [], "abstract_summary": "When pre-trained on large unsupervised textual corpora, language models are able to\nstore and retrieve factual knowledge to some extent, making it possible to use them di-\nrectly for zero-shot cloze-style question answering. However, storing factual knowledge in a\n\fxed number of weights of a language model clearly has limitations. Previous approaches\nhave successfully provided access to information outside the model weights using super-\nvised architectures that combine an information retrieval s...", "code_snippets": []}, "errors": []}
{"doc_id": "2010.02502v4.pdf", "source_path": "../data/test_docs/2010.02502v4.pdf", "fields": {"authors": [], "abstract_summary": "", "code_snippets": []}, "errors": []}
